{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SlHADAO-FnQ_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPPO2NYqJF8C",
    "outputId": "cee42c8a-03ab-4766-f405-22f9f945619c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 1.12.0+cu116 CUDA 11.6\n",
      "Device: cuda:0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_17:51:05_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.103\n",
      "Build cuda_12.3.r12.3/compiler.33492891_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, device(type='cuda'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "print('Device:', torch.device('cuda:0'))\n",
    "\n",
    "!nvcc --version\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available(), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U7EsH32EFnRA"
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "NUM_TARGETS= 2\n",
    "\n",
    "USER_PATHWAY  = [256, 128, 64]\n",
    "ITEM_PATHWAY = [256, 128, 64]\n",
    "COMBINED_PATHWAY = [256, 128, 64, 16]\n",
    "\n",
    "EMBED_DIM = 40\n",
    "NUM_ITEM_EMBED = 5850\n",
    "NUM_USER_EMBED = 105571\n",
    "NUM_CUPSIZE_EMBED =  5\n",
    "NUM_CATEGORY_EMBED = 68\n",
    "\n",
    "NUM_USER_NUMERIC = 6\n",
    "NUM_ITEM_NUMERIC = 1\n",
    "\n",
    "DROPOUT = 0.3\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 0.005\n",
    "WEIGHT_DECAY = 0.0001\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RentTheRunway(torch.utils.data.Dataset):\n",
    "    def __init__(self,datapath):\n",
    "        self.data = pd.read_csv(datapath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        r = self.data.iloc[idx,:]\n",
    "\n",
    "        return {\n",
    "            \"user_id\" : np.array(r['user_id'], dtype=np.int64),\n",
    "            \"cup_size\" : np.array(r['rating']/2-1, dtype=np.int64),\n",
    "            \"user_numeric\" : np.array([r['bust size'], r['weight'], r['height'], r['age'], r['rented_int'], r['bodytype_int']], dtype=np.float32),\n",
    "            \"item_id\" : np.array(r['item_id'], dtype = np.int64),\n",
    "            \"category\" :np.array(r['category_int']-1, dtype = np.int64),\n",
    "            \"item_numeric\" : np.array([r['size']], dtype=np.float32),\n",
    "            \"fit\" : np.array(r['fit'], dtype=np.int64)\n",
    "        }\n",
    "\n",
    "datasets = OrderedDict()\n",
    "splits = ['train', 'valid']\n",
    "datasets['train'] =  RentTheRunway(\"data/renttherunway_final_data_processed_train.csv\")\n",
    "datasets['valid'] =  RentTheRunway(\"data/renttherunway_final_data_processed_valid.csv\")\n",
    "datasets['test'] = RentTheRunway(\"data/renttherunway_final_data_processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0vxnymEpFnRD"
   },
   "outputs": [],
   "source": [
    "# macro - Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
    "# weighted - Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "def compute_metrics(target, pred_probs, averaging = \"macro\"):\n",
    "\n",
    "    pred_labels = pred_probs.argmax(-1)\n",
    "    precision = metrics.precision_score(target, pred_labels, average=averaging)\n",
    "    recall = metrics.recall_score(target, pred_labels, average=averaging)\n",
    "    f1_score = metrics.f1_score(target, pred_labels, average=averaging)\n",
    "    accuracy = metrics.accuracy_score(target, pred_labels)\n",
    "    auc = metrics.roc_auc_score(target, pred_probs[:,1], average=averaging, multi_class=\"ovr\")\n",
    "\n",
    "    return precision, recall, f1_score, accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d4tsne(datasets):\n",
    "    dataset=[]\n",
    "    y=[]\n",
    "    for id,d in enumerate(datasets['train']):\n",
    "        if id >499:break\n",
    "        data=[]\n",
    "        data.append(int(d['cup_size']))\n",
    "        for i in d['user_numeric']:\n",
    "            data.append(float(i))\n",
    "        \n",
    "        data.append(int(d['category']))\n",
    "        for i in d['item_numeric']:\n",
    "            data.append(float(i))\n",
    "        dataset.append(data)\n",
    "        y.append(1 if d['fit']==1 else 0)\n",
    "    # for d in datasets['test']:\n",
    "    #     data=[]\n",
    "    #     data.append(int(d['cup_size']))\n",
    "    #     for i in d['user_numeric']:\n",
    "    #         data.append(float(i))\n",
    "\n",
    "    #     data.append(int(d['category']))\n",
    "    #     for i in d['item_numeric']:\n",
    "    #         data.append(float(i))\n",
    "    #     dataset.append(data)\n",
    "    #     y.append(1 if d['fit']==1 else 0)\n",
    "    return dataset,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,y=d4tsne(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_visualization(X,y,name):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "    X_tsne = tsne.fit_transform(np.array(X))\n",
    "\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'yellow']\n",
    "\n",
    "    for i in np.unique(y):\n",
    "        plt.scatter(X_tsne[y == i, 0], X_tsne[y == i, 1], color=colors[i], label=i, alpha=0.3, s=5)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne_visualization(dataset,y,\"original_ds2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yicAQXX996RY"
   },
   "outputs": [],
   "source": [
    "class Base(nn.Module):\n",
    "    def __init__(self, user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.user_pathway = user_pathway\n",
    "        self.item_pathway = item_pathway\n",
    "        self.combined_pathway = combined_pathway\n",
    "        self.embedding_dim = embed_dim\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_user_embed, embed_dim, max_norm=1.0 )\n",
    "        self.cup_size_embedding = nn.Embedding(num_cupsize_embed, embed_dim, max_norm=1.0 )\n",
    "        self.item_embedding = nn.Embedding(num_item_embed, embed_dim, max_norm=1.0 )\n",
    "        self.category_embedding = nn.Embedding(num_category_embed, embed_dim, max_norm=1.0 )\n",
    "\n",
    "\n",
    "    def forward(self, batch_input):\n",
    "        # Customer Pathway\n",
    "        user_emb = self.user_embedding(batch_input[\"user_id\"])\n",
    "        cup_size_emb = self.cup_size_embedding(batch_input[\"cup_size\"])\n",
    "        user_representation = torch.cat( [user_emb, cup_size_emb, batch_input[\"user_numeric\"]], dim=-1 )\n",
    "        user_representation = self.user_transform_blocks(user_representation)\n",
    "\n",
    "        # Article Pathway\n",
    "        item_emb = self.item_embedding(batch_input[\"item_id\"])\n",
    "        category_emb = self.category_embedding(batch_input[\"category\"])\n",
    "        item_representation = torch.cat( [item_emb, category_emb, batch_input[\"item_numeric\"]], dim=-1 )\n",
    "        item_representation = self.item_transform_blocks(item_representation)\n",
    "\n",
    "        # Combine the pathways\n",
    "        combined_representation = torch.cat( [user_representation, item_representation], dim=-1 )\n",
    "        combined_representation = self.combined_blocks(combined_representation)\n",
    "\n",
    "        # Output layer of logits\n",
    "        logits = self.hidden2output(combined_representation)\n",
    "        pred_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return logits, pred_probs, combined_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PpthRqIj96RZ"
   },
   "outputs": [],
   "source": [
    "class SFNet(Base):\n",
    "    def __init__(self, user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout):\n",
    "        super().__init__(user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout)\n",
    "\n",
    "        # Customer pathway transformation  ==  user_embedding_dim + cup_size_embedding_dim + num_user_numeric_features\n",
    "        user_features_input_size = 2 * self.embedding_dim + NUM_USER_NUMERIC\n",
    "        self.user_pathway.insert(0, user_features_input_size)\n",
    "        self.user_transform_blocks = []\n",
    "        for i in range(1, len(self.user_pathway)):\n",
    "            self.user_transform_blocks.append( SkipBlock( self.user_pathway[i - 1], self.user_pathway[i] ) )\n",
    "            self.user_transform_blocks.append(nn.Dropout(DROPOUT))\n",
    "        self.user_transform_blocks = nn.Sequential(*self.user_transform_blocks)\n",
    "\n",
    "        # Article pathway transformation == item_embedding_dim + category_embedding_dim + num_item_numeric_features\n",
    "        item_features_input_size = 2 * self.embedding_dim + NUM_ITEM_NUMERIC\n",
    "        self.item_pathway.insert(0, item_features_input_size)\n",
    "        self.item_transform_blocks = []\n",
    "        for i in range(1, len(self.item_pathway)):\n",
    "            self.item_transform_blocks.append( SkipBlock( self.item_pathway[i - 1], self.item_pathway[i]) )\n",
    "            self.item_transform_blocks.append(nn.Dropout(DROPOUT))\n",
    "        self.item_transform_blocks = nn.Sequential(*self.item_transform_blocks)\n",
    "\n",
    "        # Combined top layer pathway\n",
    "        # u = output dim of user_transform_blocks, # t = output dim of item_transform_blocks\n",
    "        # Pathway combination through [u, t] # Hence, input dimension will be 2*dim(u)\n",
    "        combined_layer_input_size = 2 * self.user_pathway[-1]\n",
    "        self.combined_pathway.insert(0, combined_layer_input_size)\n",
    "        self.combined_blocks = []\n",
    "        for i in range(1, len(self.combined_pathway)):\n",
    "            self.combined_blocks.append( SkipBlock( self.combined_pathway[i - 1], self.combined_pathway[i]) )\n",
    "            self.combined_blocks.append(nn.Dropout(DROPOUT))\n",
    "        self.combined_blocks = nn.Sequential(*self.combined_blocks)\n",
    "\n",
    "        # Linear transformation from last hidden layer to output\n",
    "        self.hidden2output = nn.Linear(self.combined_pathway[-1], NUM_TARGETS)\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Skip Connection for feed-forward  - ResNet Block \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(input_dim, output_dim)\n",
    "        self.W2 = nn.Linear(output_dim, output_dim)\n",
    "        self.I = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"  z = ReLU(   W2( ReLU( W1(x))) + Projection(x))    \"\"\"\n",
    "        z = relu(self.W2(relu(self.W1(x))) + self.I(x))\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nP3EpC5EFnRE"
   },
   "outputs": [],
   "source": [
    "class MLP(Base):\n",
    "    def __init__(self,user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout):\n",
    "        super().__init__(user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout)\n",
    "\n",
    "        # Customer pathway transformation  ==  user_embedding_dim + cup_size_embedding_dim + num_user_numeric_features\n",
    "        user_features_input_size = 2 * self.embedding_dim + NUM_USER_NUMERIC\n",
    "        self.user_pathway.insert(0, user_features_input_size)\n",
    "        self.user_transform_blocks = []\n",
    "        for i in range(1, len(self.user_pathway)):\n",
    "            self.user_transform_blocks.append( LinearBlock( self.user_pathway[i - 1], self.user_pathway[i] ) )\n",
    "        self.user_transform_blocks = nn.Sequential(*self.user_transform_blocks)\n",
    "\n",
    "        # Article pathway transformation == item_embedding_dim + category_embedding_dim + num_item_numeric_features\n",
    "        item_features_input_size = 2 * self.embedding_dim + NUM_ITEM_NUMERIC\n",
    "        self.item_pathway.insert(0, item_features_input_size)\n",
    "        self.item_transform_blocks = []\n",
    "        for i in range(1, len(self.item_pathway)):\n",
    "            self.item_transform_blocks.append( LinearBlock( self.item_pathway[i - 1], self.item_pathway[i])  )\n",
    "        self.item_transform_blocks = nn.Sequential(*self.item_transform_blocks)\n",
    "\n",
    "        # Combined top layer pathway\n",
    "        # u = output dim of user_transform_blocks, # t = output dim of item_transform_blocks\n",
    "        # Pathway combination through [u, t] # Hence, input dimension will be 4*dim(u)\n",
    "        combined_layer_input_size = 2 * self.user_pathway[-1]\n",
    "        self.combined_pathway.insert(0, combined_layer_input_size)\n",
    "        self.combined_blocks = []\n",
    "        for i in range(1, len(self.combined_pathway)):\n",
    "            self.combined_blocks.append( LinearBlock( self.combined_pathway[i - 1], self.combined_pathway[i]) )\n",
    "        self.combined_blocks = nn.Sequential(*self.combined_blocks)\n",
    "\n",
    "        # Linear transformation from last hidden layer to output\n",
    "        self.hidden2output = nn.Linear(self.combined_pathway[-1], NUM_TARGETS)\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Skip Connection for feed-forward  - ResNet Block \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"  z = ReLU(   W2( ReLU( W1(x))) + Projection(x))    \"\"\"\n",
    "        return relu(self.W1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6QlmtdwFnRK",
    "outputId": "97a6415f-1198-408e-b2d8-ed5759920704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MLP(\n",
      "  (user_embedding): Embedding(105571, 40, max_norm=1.0)\n",
      "  (cup_size_embedding): Embedding(5, 40, max_norm=1.0)\n",
      "  (item_embedding): Embedding(5850, 40, max_norm=1.0)\n",
      "  (category_embedding): Embedding(68, 40, max_norm=1.0)\n",
      "  (user_transform_blocks): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (W1): Linear(in_features=86, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (item_transform_blocks): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (W1): Linear(in_features=81, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (combined_blocks): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (3): LinearBlock(\n",
      "      (W1): Linear(in_features=64, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (hidden2output): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Number of model parameters: 4660578\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP(USER_PATHWAY, ITEM_PATHWAY, COMBINED_PATHWAY, EMBED_DIM, NUM_ITEM_EMBED, NUM_USER_EMBED, NUM_CUPSIZE_EMBED, NUM_CATEGORY_EMBED, DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(model)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {total_params}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR, weight_decay= WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SN2h7vipFnRL",
    "outputId": "8ff9253e-ceb9-4206-eddb-3136fd607205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 0/1204, Loss=0.72\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.5614577531814575\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.55\n",
      "VALID Batch Stats 156/157, Loss=0.58\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.5507641434669495\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.54\n",
      "TRAIN Batch Stats 700/1204, Loss=0.52\n",
      "TRAIN Batch Stats 800/1204, Loss=0.53\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.68\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.5532859563827515\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.58\n",
      "VALID Batch Stats 156/157, Loss=0.60\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.5736963152885437\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.52\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.47\n",
      "TRAIN Batch Stats 700/1204, Loss=0.63\n",
      "TRAIN Batch Stats 800/1204, Loss=0.65\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.5583974719047546\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.5744991302490234\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.61\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.65\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.5755943655967712\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.5747011303901672\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.62\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.63\n",
      "TRAIN Batch Stats 900/1204, Loss=0.66\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.5756060481071472\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.5747068524360657\n",
      "TRAIN Batch Stats 0/1204, Loss=0.52\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.50\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.43\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.5755524635314941\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.5745388865470886\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.64\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.70\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.5756435394287109\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.574729323387146\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.64\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.57\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.5755727887153625\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.57\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.576493501663208\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.48\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.69\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.62\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.48\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.5755349397659302\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.5744730234146118\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.52\n",
      "TRAIN Batch Stats 500/1204, Loss=0.62\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.64\n",
      "TRAIN Batch Stats 800/1204, Loss=0.53\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.76\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.5756655335426331\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.574491560459137\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 300/1204, Loss=0.52\n",
      "TRAIN Batch Stats 400/1204, Loss=0.54\n",
      "TRAIN Batch Stats 500/1204, Loss=0.61\n",
      "TRAIN Batch Stats 600/1204, Loss=0.62\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.64\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.5755140781402588\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.5744598507881165\n",
      "TRAIN Batch Stats 0/1204, Loss=0.53\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 300/1204, Loss=0.66\n",
      "TRAIN Batch Stats 400/1204, Loss=0.53\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.51\n",
      "TRAIN Batch Stats 800/1204, Loss=0.66\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.84\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.5757016539573669\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.5748169422149658\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.5754904747009277\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.5744758248329163\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.51\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.575507402420044\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.5744829177856445\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.51\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.5755155682563782\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.5744609832763672\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.61\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.44\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.5753608345985413\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.5744950175285339\n",
      "TRAIN Batch Stats 0/1204, Loss=0.52\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.62\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.64\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.5754718780517578\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.5744602680206299\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 300/1204, Loss=0.54\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.51\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.77\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.5756641030311584\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.5746143460273743\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.51\n",
      "TRAIN Batch Stats 200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.50\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.38\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.5753557682037354\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.5747730135917664\n",
      "TRAIN Batch Stats 0/1204, Loss=0.66\n",
      "TRAIN Batch Stats 100/1204, Loss=0.64\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.53\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.53\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.43\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.575362503528595\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.5744612812995911\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.62\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.52\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.5754110813140869\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.5745794177055359\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.64\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.62\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.49\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.5755596160888672\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.5744785070419312\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.52\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.5754692554473877\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.5748358368873596\n",
      "TRAIN Batch Stats 0/1204, Loss=0.49\n",
      "TRAIN Batch Stats 100/1204, Loss=0.50\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.66\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.53\n",
      "TRAIN Batch Stats 600/1204, Loss=0.63\n",
      "TRAIN Batch Stats 700/1204, Loss=0.61\n",
      "TRAIN Batch Stats 800/1204, Loss=0.52\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.5755272507667542\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.5744701027870178\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.54\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.62\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.37\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.5752999186515808\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.5744596719741821\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.53\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.54\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.82\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.5756181478500366\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.574510395526886\n",
      "TRAIN Batch Stats 0/1204, Loss=0.52\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.62\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.50\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.66\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.70\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.5755586624145508\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.5748996734619141\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.64\n",
      "TRAIN Batch Stats 600/1204, Loss=0.53\n",
      "TRAIN Batch Stats 700/1204, Loss=0.62\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.5754674077033997\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.5748293995857239\n",
      "TRAIN Batch Stats 0/1204, Loss=0.53\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.54\n",
      "TRAIN Batch Stats 500/1204, Loss=0.63\n",
      "TRAIN Batch Stats 600/1204, Loss=0.49\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.63\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.82\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.5756653547286987\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.5744708776473999\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.48\n",
      "TRAIN Batch Stats 500/1204, Loss=0.61\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.575437068939209\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.5744714736938477\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.54\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.43\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.5753501057624817\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.5745164752006531\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.47\n",
      "TRAIN Batch Stats 200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.51\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.77\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.5756171941757202\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.5747184753417969\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.65\n",
      "TRAIN Batch Stats 200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.50\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.49\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.5754247903823853\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.5753499865531921\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.52\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.44\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.5753723978996277\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.5745056867599487\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.50\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.52\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.53\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.5754209756851196\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.5745440721511841\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.51\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.67\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.5754981637001038\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.5745559334754944\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.50\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.5754989981651306\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.5744732022285461\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.63\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.5754190683364868\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.574549674987793\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.64\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.5754097104072571\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.5744620561599731\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.61\n",
      "TRAIN Batch Stats 800/1204, Loss=0.52\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.5754151344299316\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.5744707584381104\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.50\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.56\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.70\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.5755719542503357\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.5745102763175964\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.64\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.5754864811897278\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.5745477676391602\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.54\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.5755002498626709\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.5744599103927612\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.69\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.71\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.5754687190055847\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.5744600296020508\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.53\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.50\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.5754851698875427\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.5745311379432678\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.66\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.575529158115387\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.5744803547859192\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.51\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.61\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.5754527449607849\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.5744604468345642\n",
      "TRAIN Batch Stats 0/1204, Loss=0.64\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.67\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.5754408836364746\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.5745187997817993\n",
      "TRAIN Batch Stats 0/1204, Loss=0.51\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.52\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.65\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.5755162835121155\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.5746837854385376\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.53\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.65\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.5755448937416077\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.5745026469230652\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.46\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.50\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.5754296779632568\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.5745911002159119\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.75\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.5756237506866455\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.5746548771858215\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.52\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.53\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.48\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.5754799842834473\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.5745032429695129\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.64\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.5754877924919128\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.5744597315788269\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.66\n",
      "TRAIN Batch Stats 200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 300/1204, Loss=0.64\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.63\n",
      "TRAIN Batch Stats 900/1204, Loss=0.66\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.43\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.5753798484802246\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.5744755268096924\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.51\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.575445830821991\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.574540376663208\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.50\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.53\n",
      "TRAIN Batch Stats 800/1204, Loss=0.56\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.5756086707115173\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.5744611620903015\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.62\n",
      "TRAIN Batch Stats 400/1204, Loss=0.48\n",
      "TRAIN Batch Stats 500/1204, Loss=0.63\n",
      "TRAIN Batch Stats 600/1204, Loss=0.62\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.44\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.5753711462020874\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.5745935440063477\n",
      "TRAIN Batch Stats 0/1204, Loss=0.63\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.54\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.53\n",
      "TRAIN Batch Stats 600/1204, Loss=0.62\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.81\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.5756261944770813\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.5747871398925781\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.54\n",
      "TRAIN Batch Stats 500/1204, Loss=0.62\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.575525164604187\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.5745093822479248\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for d in datasets:\n",
    "        for split in splits:\n",
    "            data_loader = DataLoader( dataset=datasets[split], batch_size=BATCH_SIZE, shuffle = (split == \"train\") )\n",
    "\n",
    "            loss_tracker = defaultdict(tensor)\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                target_tracker = []\n",
    "                pred_tracker = []\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits, pred_probs,_ = model(batch)\n",
    "\n",
    "                # loss calculation\n",
    "                loss = loss_criterion(logits, batch[\"fit\"])   # batch['fit'] are the true labels\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                # bookkeepeing\n",
    "                loss_tracker[\"Total Loss\"] = torch.cat((loss_tracker[\"Total Loss\"], loss.view(1)))\n",
    "\n",
    "                if iteration % 100 == 0 or iteration + 1 == len(data_loader):\n",
    "                    print(f\"{split.upper()} Batch Stats {iteration}/{len(data_loader)}, Loss={loss.item() :.2f}\")\n",
    "\n",
    "                if split == \"valid\":\n",
    "                    target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "                    pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "            print( f\"{split.upper()} Epoch {epoch + 1} / {EPOCHS}, Mean Total Loss {torch.mean(loss_tracker['Total Loss'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VopBWrV5FnRM",
    "outputId": "f3856a2d-9d39-41b7-b0aa-6158df08ba8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data ...\n",
      "Evaluating model on test data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch_DL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Metrics:\n",
      " Precision = 0.5443163049167504\n",
      " Recall = 0.7377779509559435\n",
      " F1-score = 0.6264509278844569\n",
      " Accuracy = 0.7377779509559435\n",
      " AUC = 0.5\n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "target_tracker = []\n",
    "pred_tracker = []\n",
    "\n",
    "print(\"Preparing test data ...\")\n",
    "\n",
    "data_loader = DataLoader(dataset = datasets['train'], batch_size = BATCH_SIZE, shuffle=False)\n",
    "embeds1=[]\n",
    "y1=[]\n",
    "for d in datasets['train']:\n",
    "    y1.append(int(d['fit']))\n",
    "print(\"Evaluating model on test data ...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, pred_probs,embeddings = model(batch)\n",
    "        embeds1.append(embeddings.cpu())\n",
    "        # print(batch[\"fit\"].cpu().numpy())\n",
    "        target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "        # print(pred_probs.cpu().data.numpy())\n",
    "        pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "target_tracker = np.stack(target_tracker[:-1]).reshape(-1)\n",
    "pred_tracker = np.stack(pred_tracker[:-1], axis=0).reshape(-1, NUM_TARGETS)\n",
    "\n",
    "# print(target_tracker)\n",
    "# print(pred_tracker)\n",
    "precision, recall, f1_score, accuracy, auc = compute_metrics(target_tracker, pred_tracker, averaging = \"weighted\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metrics:\\n Precision = {precision}\\n Recall = {recall}\\n F1-score = {f1_score}\\n Accuracy = {accuracy}\\n AUC = {auc}\\n \")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVl5-Yfs96Rc",
    "outputId": "086fb05e-46a4-439c-d5ab-1fb51398dab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "SFNet(\n",
      "  (user_embedding): Embedding(105571, 40, max_norm=1.0)\n",
      "  (cup_size_embedding): Embedding(5, 40, max_norm=1.0)\n",
      "  (item_embedding): Embedding(5850, 40, max_norm=1.0)\n",
      "  (category_embedding): Embedding(68, 40, max_norm=1.0)\n",
      "  (user_transform_blocks): Sequential(\n",
      "    (0): SkipBlock(\n",
      "      (W1): Linear(in_features=86, out_features=86, bias=True)\n",
      "      (W2): Linear(in_features=86, out_features=86, bias=True)\n",
      "      (I): Linear(in_features=86, out_features=86, bias=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): SkipBlock(\n",
      "      (W1): Linear(in_features=86, out_features=256, bias=True)\n",
      "      (W2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (I): Linear(in_features=86, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): SkipBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (W2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (item_transform_blocks): Sequential(\n",
      "    (0): SkipBlock(\n",
      "      (W1): Linear(in_features=81, out_features=81, bias=True)\n",
      "      (W2): Linear(in_features=81, out_features=81, bias=True)\n",
      "      (I): Linear(in_features=81, out_features=81, bias=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): SkipBlock(\n",
      "      (W1): Linear(in_features=81, out_features=256, bias=True)\n",
      "      (W2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (I): Linear(in_features=81, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): SkipBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (W2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (combined_blocks): Sequential(\n",
      "    (0): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (W2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): SkipBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (W2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): SkipBlock(\n",
      "      (W1): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (W2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (I): Linear(in_features=64, out_features=16, bias=True)\n",
      "    )\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (hidden2output): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Number of model parameters: 5212934\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model = SFNet(USER_PATHWAY, ITEM_PATHWAY, COMBINED_PATHWAY, EMBED_DIM, NUM_ITEM_EMBED, NUM_USER_EMBED, NUM_CUPSIZE_EMBED, NUM_CATEGORY_EMBED, DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(model)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {total_params}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR, weight_decay= WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gon_ggoD96Rd",
    "outputId": "a8ed87e8-b9cc-4ced-d8e8-7b6dd4761aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 0/1204, Loss=0.72\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.65\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.52\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.5781539082527161\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.574590802192688\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.47\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.64\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.49\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.5754349827766418\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.574727475643158\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.51\n",
      "TRAIN Batch Stats 700/1204, Loss=0.61\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.5754599571228027\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.5744606852531433\n",
      "TRAIN Batch Stats 0/1204, Loss=0.51\n",
      "TRAIN Batch Stats 100/1204, Loss=0.64\n",
      "TRAIN Batch Stats 200/1204, Loss=0.64\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.61\n",
      "TRAIN Batch Stats 500/1204, Loss=0.61\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.65\n",
      "TRAIN Batch Stats 800/1204, Loss=0.51\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.575507402420044\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.5744605660438538\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.52\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.53\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.5754866600036621\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.5745880007743835\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.44\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.37\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.5752943754196167\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.5744789242744446\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.64\n",
      "TRAIN Batch Stats 800/1204, Loss=0.65\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.5754830241203308\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.5748172402381897\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.5754537582397461\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.5745680332183838\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.52\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.68\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.5755672454833984\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.57\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.575949490070343\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.62\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.49\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.5754833817481995\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.5744733810424805\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.65\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.43\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.5753963589668274\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.5744611620903015\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.54\n",
      "TRAIN Batch Stats 400/1204, Loss=0.54\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.76\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.5756075382232666\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.5744855999946594\n",
      "TRAIN Batch Stats 0/1204, Loss=0.63\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.54\n",
      "TRAIN Batch Stats 400/1204, Loss=0.61\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.64\n",
      "TRAIN Batch Stats 700/1204, Loss=0.60\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.75\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.5756157636642456\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.5745935440063477\n",
      "TRAIN Batch Stats 0/1204, Loss=0.63\n",
      "TRAIN Batch Stats 100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.52\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.5754419565200806\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.5744646787643433\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.71\n",
      "TRAIN Batch Stats 800/1204, Loss=0.66\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.5755521059036255\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.574684202671051\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.52\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.76\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.5756714344024658\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.5744617581367493\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.52\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.50\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.68\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.5755460262298584\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.5751504302024841\n",
      "TRAIN Batch Stats 0/1204, Loss=0.64\n",
      "TRAIN Batch Stats 100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.50\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.5755269527435303\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.5744611620903015\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.53\n",
      "TRAIN Batch Stats 500/1204, Loss=0.63\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.53\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.82\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.5756462812423706\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.5744855999946594\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.69\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.37\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.5753422379493713\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.5744612216949463\n",
      "TRAIN Batch Stats 0/1204, Loss=0.68\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.54\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.61\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.53\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.5755906701087952\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.5744625926017761\n",
      "TRAIN Batch Stats 0/1204, Loss=0.64\n",
      "TRAIN Batch Stats 100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.63\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.5754275321960449\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.5748105645179749\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.63\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.5755519866943359\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.5744651556015015\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.51\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.5755491256713867\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.5745809674263\n",
      "TRAIN Batch Stats 0/1204, Loss=0.51\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.70\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.5755392909049988\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.5745011568069458\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 300/1204, Loss=0.52\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.5755649209022522\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.574524462223053\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.61\n",
      "TRAIN Batch Stats 600/1204, Loss=0.59\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.66\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.67\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.70\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.5755701065063477\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.5747115015983582\n",
      "TRAIN Batch Stats 0/1204, Loss=0.67\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.60\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.51\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.5755446553230286\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.5745983719825745\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.63\n",
      "TRAIN Batch Stats 400/1204, Loss=0.49\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.48\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.49\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.575447678565979\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.574461042881012\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.68\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.53\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.5755453109741211\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.5744861364364624\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.64\n",
      "TRAIN Batch Stats 500/1204, Loss=0.66\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.61\n",
      "TRAIN Batch Stats 800/1204, Loss=0.57\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.90\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.5757410526275635\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.574542760848999\n",
      "TRAIN Batch Stats 0/1204, Loss=0.65\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.65\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.5755043029785156\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.5745825171470642\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.52\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.45\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.5756368637084961\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.5746985673904419\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.55\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.69\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.5754365921020508\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.5744959712028503\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.74\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.66\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.50\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.65\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.575503945350647\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.575005054473877\n",
      "TRAIN Batch Stats 0/1204, Loss=0.60\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.64\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.5754961371421814\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.5744693279266357\n",
      "TRAIN Batch Stats 0/1204, Loss=0.53\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 300/1204, Loss=0.65\n",
      "TRAIN Batch Stats 400/1204, Loss=0.60\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.64\n",
      "TRAIN Batch Stats 700/1204, Loss=0.64\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.5754504799842834\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.5744657516479492\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 300/1204, Loss=0.60\n",
      "TRAIN Batch Stats 400/1204, Loss=0.64\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.56\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.5755672454833984\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.5745999217033386\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.54\n",
      "TRAIN Batch Stats 800/1204, Loss=0.56\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.44\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.5753589272499084\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.5745159387588501\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.62\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.60\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.37\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.5753433108329773\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.5744596123695374\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.64\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.60\n",
      "TRAIN Batch Stats 800/1204, Loss=0.56\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.89\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.575707197189331\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.5744634866714478\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.53\n",
      "TRAIN Batch Stats 400/1204, Loss=0.63\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.62\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.50\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.68\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.575477659702301\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.5745151042938232\n",
      "TRAIN Batch Stats 0/1204, Loss=0.52\n",
      "TRAIN Batch Stats 100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.57\n",
      "TRAIN Batch Stats 600/1204, Loss=0.63\n",
      "TRAIN Batch Stats 700/1204, Loss=0.52\n",
      "TRAIN Batch Stats 800/1204, Loss=0.65\n",
      "TRAIN Batch Stats 900/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.5754650831222534\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.5744630098342896\n",
      "TRAIN Batch Stats 0/1204, Loss=0.62\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.66\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.52\n",
      "TRAIN Batch Stats 600/1204, Loss=0.50\n",
      "TRAIN Batch Stats 700/1204, Loss=0.59\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.5754660367965698\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.5744751691818237\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.56\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.53\n",
      "TRAIN Batch Stats 700/1204, Loss=0.61\n",
      "TRAIN Batch Stats 800/1204, Loss=0.46\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.5754663944244385\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.5746108293533325\n",
      "TRAIN Batch Stats 0/1204, Loss=0.58\n",
      "TRAIN Batch Stats 100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.63\n",
      "TRAIN Batch Stats 400/1204, Loss=0.57\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.55\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.48\n",
      "TRAIN Batch Stats 900/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.68\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.47\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.38\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.5753164291381836\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.5745828747749329\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.64\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.67\n",
      "TRAIN Batch Stats 500/1204, Loss=0.46\n",
      "TRAIN Batch Stats 600/1204, Loss=0.53\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.63\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.5755060911178589\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.5746645927429199\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.55\n",
      "TRAIN Batch Stats 500/1204, Loss=0.60\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.53\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.51\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.5754595994949341\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.5748395919799805\n",
      "TRAIN Batch Stats 0/1204, Loss=0.57\n",
      "TRAIN Batch Stats 100/1204, Loss=0.55\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.49\n",
      "TRAIN Batch Stats 500/1204, Loss=0.63\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.63\n",
      "TRAIN Batch Stats 800/1204, Loss=0.50\n",
      "TRAIN Batch Stats 900/1204, Loss=0.50\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.5755978226661682\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.5744630098342896\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.54\n",
      "TRAIN Batch Stats 200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.64\n",
      "TRAIN Batch Stats 500/1204, Loss=0.65\n",
      "TRAIN Batch Stats 600/1204, Loss=0.52\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.54\n",
      "TRAIN Batch Stats 900/1204, Loss=0.46\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.69\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.5755600929260254\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.574470043182373\n",
      "TRAIN Batch Stats 0/1204, Loss=0.55\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.62\n",
      "TRAIN Batch Stats 300/1204, Loss=0.58\n",
      "TRAIN Batch Stats 400/1204, Loss=0.62\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.55\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.5755152702331543\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.5744791030883789\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 300/1204, Loss=0.67\n",
      "TRAIN Batch Stats 400/1204, Loss=0.64\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.62\n",
      "TRAIN Batch Stats 700/1204, Loss=0.63\n",
      "TRAIN Batch Stats 800/1204, Loss=0.58\n",
      "TRAIN Batch Stats 900/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.5754907131195068\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.5747803449630737\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.62\n",
      "TRAIN Batch Stats 400/1204, Loss=0.52\n",
      "TRAIN Batch Stats 500/1204, Loss=0.59\n",
      "TRAIN Batch Stats 600/1204, Loss=0.57\n",
      "TRAIN Batch Stats 700/1204, Loss=0.57\n",
      "TRAIN Batch Stats 800/1204, Loss=0.60\n",
      "TRAIN Batch Stats 900/1204, Loss=0.49\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.575416088104248\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.5745177865028381\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.61\n",
      "TRAIN Batch Stats 200/1204, Loss=0.53\n",
      "TRAIN Batch Stats 300/1204, Loss=0.56\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.56\n",
      "TRAIN Batch Stats 600/1204, Loss=0.54\n",
      "TRAIN Batch Stats 700/1204, Loss=0.56\n",
      "TRAIN Batch Stats 800/1204, Loss=0.64\n",
      "TRAIN Batch Stats 900/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.49\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.56\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.5754547715187073\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.5744713544845581\n",
      "TRAIN Batch Stats 0/1204, Loss=0.54\n",
      "TRAIN Batch Stats 100/1204, Loss=0.47\n",
      "TRAIN Batch Stats 200/1204, Loss=0.55\n",
      "TRAIN Batch Stats 300/1204, Loss=0.64\n",
      "TRAIN Batch Stats 400/1204, Loss=0.56\n",
      "TRAIN Batch Stats 500/1204, Loss=0.53\n",
      "TRAIN Batch Stats 600/1204, Loss=0.53\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.50\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.5753904581069946\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.5744785666465759\n",
      "TRAIN Batch Stats 0/1204, Loss=0.61\n",
      "TRAIN Batch Stats 100/1204, Loss=0.58\n",
      "TRAIN Batch Stats 200/1204, Loss=0.54\n",
      "TRAIN Batch Stats 300/1204, Loss=0.61\n",
      "TRAIN Batch Stats 400/1204, Loss=0.61\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.66\n",
      "TRAIN Batch Stats 800/1204, Loss=0.62\n",
      "TRAIN Batch Stats 900/1204, Loss=0.65\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.5754976272583008\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.5744616389274597\n",
      "TRAIN Batch Stats 0/1204, Loss=0.53\n",
      "TRAIN Batch Stats 100/1204, Loss=0.57\n",
      "TRAIN Batch Stats 200/1204, Loss=0.59\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.58\n",
      "TRAIN Batch Stats 500/1204, Loss=0.52\n",
      "TRAIN Batch Stats 600/1204, Loss=0.58\n",
      "TRAIN Batch Stats 700/1204, Loss=0.53\n",
      "TRAIN Batch Stats 800/1204, Loss=0.51\n",
      "TRAIN Batch Stats 900/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.52\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.67\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.76\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.5757018327713013\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.5744912028312683\n",
      "TRAIN Batch Stats 0/1204, Loss=0.56\n",
      "TRAIN Batch Stats 100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 200/1204, Loss=0.63\n",
      "TRAIN Batch Stats 300/1204, Loss=0.57\n",
      "TRAIN Batch Stats 400/1204, Loss=0.59\n",
      "TRAIN Batch Stats 500/1204, Loss=0.58\n",
      "TRAIN Batch Stats 600/1204, Loss=0.56\n",
      "TRAIN Batch Stats 700/1204, Loss=0.55\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.55\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.59\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.61\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.81\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.5756398439407349\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.5747784376144409\n",
      "TRAIN Batch Stats 0/1204, Loss=0.59\n",
      "TRAIN Batch Stats 100/1204, Loss=0.60\n",
      "TRAIN Batch Stats 200/1204, Loss=0.64\n",
      "TRAIN Batch Stats 300/1204, Loss=0.63\n",
      "TRAIN Batch Stats 400/1204, Loss=0.54\n",
      "TRAIN Batch Stats 500/1204, Loss=0.54\n",
      "TRAIN Batch Stats 600/1204, Loss=0.54\n",
      "TRAIN Batch Stats 700/1204, Loss=0.58\n",
      "TRAIN Batch Stats 800/1204, Loss=0.61\n",
      "TRAIN Batch Stats 900/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.56\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.63\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.57551109790802\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.5744602084159851\n",
      "TRAIN Batch Stats 0/1204, Loss=0.53\n",
      "TRAIN Batch Stats 100/1204, Loss=0.62\n",
      "TRAIN Batch Stats 200/1204, Loss=0.52\n",
      "TRAIN Batch Stats 300/1204, Loss=0.59\n",
      "TRAIN Batch Stats 400/1204, Loss=0.69\n",
      "TRAIN Batch Stats 500/1204, Loss=0.55\n",
      "TRAIN Batch Stats 600/1204, Loss=0.50\n",
      "TRAIN Batch Stats 700/1204, Loss=0.65\n",
      "TRAIN Batch Stats 800/1204, Loss=0.59\n",
      "TRAIN Batch Stats 900/1204, Loss=0.54\n",
      "TRAIN Batch Stats 1000/1204, Loss=0.57\n",
      "TRAIN Batch Stats 1100/1204, Loss=0.53\n",
      "TRAIN Batch Stats 1200/1204, Loss=0.60\n",
      "TRAIN Batch Stats 1203/1204, Loss=0.43\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.5753874778747559\n",
      "VALID Batch Stats 0/157, Loss=0.62\n",
      "VALID Batch Stats 100/157, Loss=0.59\n",
      "VALID Batch Stats 156/157, Loss=0.56\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.5744605660438538\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for d in datasets:\n",
    "        for split in splits:\n",
    "            data_loader = DataLoader( dataset=datasets[split], batch_size=BATCH_SIZE, shuffle = (split == \"train\") )\n",
    "\n",
    "            loss_tracker = defaultdict(tensor)\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                target_tracker = []\n",
    "                pred_tracker = []\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits, pred_probs,_ = model(batch)\n",
    "\n",
    "                # loss calculation\n",
    "                loss = loss_criterion(logits, batch[\"fit\"])   # batch['fit'] are the true labels\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                # bookkeepeing\n",
    "                loss_tracker[\"Total Loss\"] = torch.cat((loss_tracker[\"Total Loss\"], loss.view(1)))\n",
    "\n",
    "                if iteration % 100 == 0 or iteration + 1 == len(data_loader):\n",
    "                    print(f\"{split.upper()} Batch Stats {iteration}/{len(data_loader)}, Loss={loss.item() :.2f}\")\n",
    "\n",
    "                if split == \"valid\":\n",
    "                    target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "                    pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "            print( f\"{split.upper()} Epoch {epoch + 1} / {EPOCHS}, Mean Total Loss {torch.mean(loss_tracker['Total Loss'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfZJOYqG96Rd",
    "outputId": "5932e070-1ed1-466b-a1a5-b7274e2386a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data ...\n",
      "Evaluating model on test data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch_DL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Metrics:\n",
      " Precision = 0.5443163049167504\n",
      " Recall = 0.7377779509559435\n",
      " F1-score = 0.6264509278844569\n",
      " Accuracy = 0.7377779509559435\n",
      " AUC = 0.5\n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "target_tracker = []\n",
    "pred_tracker = []\n",
    "embeds2=[]\n",
    "\n",
    "y2=[]\n",
    "for d in datasets['train']:\n",
    "    y2.append(int(d['fit']))\n",
    "print(\"Preparing test data ...\")\n",
    "\n",
    "data_loader = DataLoader(dataset = datasets['train'], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Evaluating model on test data ...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, pred_probs,embeddings = model(batch)\n",
    "        embeds2.append(embeddings.cpu())\n",
    "        target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "        pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "target_tracker = np.stack(target_tracker[:-1]).reshape(-1)\n",
    "pred_tracker = np.stack(pred_tracker[:-1], axis=0).reshape(-1, NUM_TARGETS)\n",
    "precision, recall, f1_score, accuracy, auc = compute_metrics(target_tracker, pred_tracker, averaging = \"weighted\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metrics:\\n Precision = {precision}\\n Recall = {recall}\\n F1-score = {f1_score}\\n Accuracy = {accuracy}\\n AUC = {auc}\\n \")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=[1 if d==1 else 0 for d in y1]\n",
    "y2=[1 if d==1 else 0 for d in y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=[]\n",
    "d2=[]\n",
    "for e in embeds1:\n",
    "    for ee in e.tolist():\n",
    "        d1.append(ee)\n",
    "for e in embeds2:\n",
    "    for ee in e.tolist():\n",
    "        d2.append(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_visualization(d1[:500],y1[:500],\"model1_ds2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_visualization(d2[:500],y2[:500],\"model2_ds2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 特征缩放\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 创建 SVM 分类器\n",
    "clf = SVC(kernel='linear')  # 这里使用线性核，您也可以尝试 'rbf', 'poly' 等其他核\n",
    "\n",
    "# 训练模型\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 显示结果\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1e5d2acd1631045f09d34790d51dde8c5f13a0de3f2ba4add1e385dbc0b204e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
